# Multilayer Perceptron (MLP) from Scratch

This project implements a basic Multilayer Perceptron (MLP) for digit classification on the MNIST dataset using NumPy. The implementation allows modification 
of the model’s architecture (number of layers and neurons per layer), learning rate, number of epochs, and mini-batch size for stochastic gradient descent.


## Dataset

The model is trained and evaluated on the [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist) of handwritten digits, loaded via `keras.datasets`.


## Acknowledgements

This project was built with inspiration and guidance from the following excellent resources:
- [3Blue1Brown’s “Neural Networks” series on YouTube](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Michael Nielsen’s online book "Neural Networks and Deep Learning"](http://neuralnetworksanddeeplearning.com/chap1.html)
